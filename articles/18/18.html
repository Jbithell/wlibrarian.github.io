
<!DOCTYPE html>
<html lang="en-GB">

<head>
    <title>Librarian Online - 
Adventures in Recreational Mathematics IV: Big-O notation
</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="https://librarian.cf/stylesheets/main.css">
    <link rel="stylesheet" type="text/css" href="https://librarian.cf/stylesheets/article.css">
	<link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i" rel="stylesheet">
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>

<body>
    <header>
        <div id='top'>
			<a href="https://librarian.cf">
			<div id='logo'>
				<img src="https://librarian.cf/media/logo.svg" id='logoimg'>
			</div>
        	</a>
        <nav>
            <a href="https://librarian.cf/" class="nav">Home</a> &vellip;
            <a href="https://librarian.cf/d.html" class="nav">Dialectic</a> &vellip;
            <a href="https://librarian.cf/r.html" class="nav">Review</a> &vellip;
            <a href="https://librarian.cf/s.html" class="nav">Sciences and mathematics</a> &vellip;
            <a href="https://librarian.cf/i.html" class="nav">Issues</a> &vellip;
            <a href="https://librarian.cf/l.html" class="nav">Letters</a> &vellip;
            <a href="https://librarian.cf/e.html" class="nav">Editorial</a>
        </nav>
        </div>
    </header>

    <main>
        <aside class="fill"></aside>
        <aside class="meta provenance">
<a class='nav' href='https://librarian.cf/authors/benedict-randall-shaw.html'>Benedict Randall Shaw</a>
4.9.17

        </aside>
        <article>
<h1 id="adventures-in-recreational-mathematics-iv-big-o-notation">Adventures in Recreational Mathematics IV: Big-O notation</h1>

<p>DISCLAIMER: this edition of ARM contains neither traditional maths nor any pretty pictures; rather, we are stepping out into the related field of computer science, and are stuck with graphs. We apologise for any inconvenience caused.</p>
<h1 id="big-o-notation">Big-O Notation</h1>
<p>In computer science, one seeks to write efficient algorithms, so as not to waste valuable processing time. However, as counterintuitive as it sounds, what we care about most is not how well a program performs on a given value, but rather how well it scales to more complex situations.<br />
<img src="smallgraph" alt="image" /></p>
<p><img src="mediumgraph" alt="image" /></p>
<p><img src="largegraph" alt="image" /></p>
<p>Consider three programs which sort a list of length <span class="math inline">\(n\)</span>, which we’ll call the red, blue, and green programs. We now graph the worst-case (longest) period of time required for each of these programs against the lengths of list, shown at different scales in Figures 1, 2, and 3. From Figure 1, it would seem that the green program is the quickest; however, for most values it turns out to be by far the worst. We don’t usually care about comparing the time taken to sort short lists, as this can be done very quickly either way, and the differences in time taken aren’t actually that large. What we care about are cases where the list is long, as shown on the far right of Figure 3, as the slowest programs can take many times to longer to run than the fastest. In fact, in many such problems, even algorithms which both look at the small scale to be similar (e.g. the blue and green programs) grow to be have very different performances eventually. Computer scientists describe the rate of growth of running times using <em>big-O notation</em>.</p>
<p>We say that <span class="math inline">\(f(n)=O(g(n))\)</span>, where <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are functions on the real numbers, if there exist positive real numbers <span class="math inline">\(M\)</span> and <span class="math inline">\(y\)</span> such that for all values of <span class="math inline">\(n \geq{} y\)</span>, <span class="math inline">\(|f(n)|\leq{}|Mg(n)|\)</span>. At face value, it may seem that by making <span class="math inline">\(M\)</span> very large, any program must take less than <span class="math inline">\(Mg(n)\)</span> units of time to run. This is not the case, as can be illustrated with <span class="math inline">\(f(n)=n^2\)</span> and <span class="math inline">\(g(n)=n\)</span>; for any value of <span class="math inline">\(M\)</span>, for all values of <span class="math inline">\(n&gt;M\)</span>, <span class="math inline">\(f(n)=n^2&gt;Mn=Mg(n)\)</span> as <span class="math inline">\(n&gt;M\)</span>, so <span class="math inline">\(f(n)&gt;Mg(n)\)</span> eventually for all values of <span class="math inline">\(M\)</span>. What we really mean in a nutshell by <span class="math inline">\(f(n)=O(g(n))\)</span> is that <span class="math inline">\(f(n)\)</span> doesn’t grow more quickly than <span class="math inline">\(g(n)\)</span><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. There are a variety of other pieces of big-O notation. For example, we say <span class="math inline">\(f(n)=\Omega(g(n))\)</span> if there exist positive real numbers <span class="math inline">\(N\)</span> and <span class="math inline">\(z\)</span> such that for all values of <span class="math inline">\(n \geq{} z\)</span>, <span class="math inline">\(|f(n)|\geq{}|Ng(n)|\)</span>; this essentially means that <span class="math inline">\(f(n)\)</span> doesn’t grow less quickly than <span class="math inline">\(g(n)\)</span>. We also say that <span class="math inline">\(f(n)=\Theta(g(n))\)</span> if <span class="math inline">\(f(n)=O(g(n))\)</span> and <span class="math inline">\(f(n)=\Omega(g(n))\)</span> (that is to say, if <span class="math inline">\(f(n)\)</span> grows roughly as quickly as <span class="math inline">\(g(n)\)</span>).</p>
<p>Interestingly, if a function <span class="math inline">\(f(n)=g(n)+h(n)\)</span>, and <span class="math inline">\(g(n)\)</span> grows at least as quickly as <span class="math inline">\(h(n)\)</span>, and <span class="math inline">\(g(n)=O(d(n))\)</span> for some function <span class="math inline">\(d\)</span>, then as past a certain point <span class="math inline">\(g(n)\geq{}h(n)\)</span>, and past said point <span class="math inline">\(f(n)\leq{}2g(n)\leq2Md(n)\)</span>, so <span class="math inline">\(f(n)=O(d(n))\)</span> too (as <span class="math inline">\(f(n)\leq{}2Md(n)\)</span> and <span class="math inline">\(2M\)</span> is a constant real number). What this really means is that a function grows as quickly as its fastest growing term; for example, if <span class="math inline">\(f(n)=2x^3 + 5n^2 + 2\log(n) + 11\)</span>, then <span class="math inline">\(f(n)=O(x^3)\)</span> as <span class="math inline">\(2x^3\)</span> grows more quickly than the other terms. For this, it is helpful to have some idea about which functions grow more quickly than others; a chart of such functions is given at the end of this section.</p>
<p>In computer science, we care predominantly about the worst-case running time; it (usually) causes no problems if a program runs quickly, but a program taking ages to run is to be avoided if the program is to be usable. We therefore tend to prioritise <span class="math inline">\(O\)</span> over <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(\Theta\)</span>. We say a program has <em>time complexity</em> of <span class="math inline">\(O(g(n))\)</span> (or sometimes we just say that it is <span class="math inline">\(O(g(n))\)</span>) if its worst case running time in arbitrary time units is <span class="math inline">\(O(g(n))\)</span>; for example, the red program in Figures 1–3 is linear and so has time complexity <span class="math inline">\(O(n)\)</span>. (It is also technically <span class="math inline">\(O(n^2)\)</span>, as <span class="math inline">\(n^2\)</span> grows faster than a linear function, but this is unhelpful, so the time complexity uses the slowest growing function we can possibly place inside the brackets of <span class="math inline">\(O()\)</span>.) We have names for some time complexities; for example, we say the red program has linear time complexity, for obvious reasons. Below is a chart of common time complexities and their names, ordered from slowest to fastest growing.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(O(1)\)</span></th>
<th align="center">constant</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(O(\log*n))\)</span></td>
<td align="center">log-star</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(O(\log(\log{}n))\)</span></td>
<td align="center">double logarithmic</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(O(\log{}n)\)</span></td>
<td align="center">logarithmic</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(O(n^c)\)</span> where <span class="math inline">\(0&lt;c&lt;1\)</span> is a constant</td>
<td align="center">fractional power</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(O(n)\)</span></td>
<td align="center">linear</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(O(n\log{}n)\)</span></td>
<td align="center">linearithmic</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(O(n^c)\)</span> where <span class="math inline">\(1&lt;c&lt;2\)</span> is a constant</td>
<td align="center">polynomial</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(O(n^2)\)</span></td>
<td align="center">quadratic (also polynomial)</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(O(n^c)\)</span> where <span class="math inline">\(2&lt;c&lt;3\)</span> is a constant</td>
<td align="center">polynomial</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(O(n^3)\)</span></td>
<td align="center">cubic (also polynomial)</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(O(n^c)\)</span> where <span class="math inline">\(3&lt;c\)</span> is a constant</td>
<td align="center">polynomial</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(O(c^n)\)</span> where <span class="math inline">\(1&lt;c\)</span> is a constant</td>
<td align="center">exponential</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(O(n!)\)</span></td>
<td align="center">factorial</td>
</tr>
</tbody>
</table>
<h1 id="long-multiplication">Long multiplication</h1>
<p>It is necessary for a computer to be able to carry out arithmetic operations as quickly as possible, as these are the building blocks of most programs. We can add two <span class="math inline">\(n\)</span>-bit<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> numbers (those less than <span class="math inline">\(2^n\)</span>) with <span class="math inline">\(O(n)\)</span> time complexity on a sequential circuit through the usual method that one is taught in school; one simply goes through the two numbers bit by bit, starting with the least significant, and adds them, carrying over into the next column if one needs to. One must only ever add a maximum of three bits (two from the numbers and one from carrying) each time, each individual bit addition has constant time complexity. Because one goes through <span class="math inline">\(n\)</span> such sets of bits, addition has <span class="math inline">\(O(n)\)</span> time complexity.</p>
<p>Multiplication is trickier. However, in the same way that we find it very easy to multiply by powers of 10 by adding zeroes, modern computers are able to multiply by powers of two with <span class="math inline">\(O(1)\)</span> time complexity (using a circuit called a barrel shifter which will not be explained here). This is called a bit shift, and is very helpful. It means that the long multiplication algorithm, which probably we have all used at some point or other, can be done fairly quickly on a computer. For two <span class="math inline">\(n\)</span>-bit numbers, long multiplication gives us an intermediate term for each “1” bit of the first of the numbers to be multiplied. In the worst case scenario, there are <span class="math inline">\(n\)</span> of these terms, and as each is produced in constant time from bit shifts, the production of these has <span class="math inline">\(O(n)\)</span> time complexity.</p>
<p>As they are all the product of an <span class="math inline">\(n\)</span>-bit number and a power of two that fits in <span class="math inline">\(n\)</span>-bits, the terms fit in <span class="math inline">\(2n\)</span> bits. Thus we are adding <span class="math inline">\(n\)</span> <span class="math inline">\(2n\)</span>-bit numbers, so we must perform <span class="math inline">\(n-1\)</span> addition operations. As addition is <span class="math inline">\(O(n)\)</span>, this means that the summation of the intermediate terms has <span class="math inline">\((n-1)\times{}O(2n)=O(2n^2-2n)=O(n^2)\)</span> time complexity. The production of the terms is <span class="math inline">\(O(n)\)</span>, so the time complexity of long multiplication is <span class="math inline">\(O(n)+O(n^2)=O(n^2)\)</span>. This is pretty bad. If we wanted to multiply two ten-million-digit numbers (33 million bits), and we were able to perform a billion single-bit additions a second, this algorithm would take nearly a fortnight.</p>
<h1 id="karatsubas-algorithm">Karatsuba’s algorithm</h1>
<p>It was conjectured by Andrey Kolmogorov, a notable Soviet mathematician, that the quadratic time complexity of long multiplication could not be improved upon. In 1960, he held a seminar at which he mentioned this, among other computational complexity problems. A week later, Anatoly Karatsuba, a 23-year-old student, found this algorithm, which runs with <span class="math inline">\(O(n^{log_{2}3})\approx{}O(n^{1.585})\)</span> time complexity.</p>
<p>It belongs to a class of algorithms known as “divide and conquer” algorithms. These work by splitting problems into smaller instances of the same type of problem, until they can be solved easily, and then recombining the solutions to solve the initial problem.</p>
<p>Consider multiplying two <span class="math inline">\(n\)</span>-bit integers, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Let <span class="math inline">\(m\)</span> be <span class="math inline">\(\left\lceil\frac{n}{2}\right\rceil\)</span>. Then by splitting <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> down the middle, we can obtain <span class="math inline">\(x_1,x_0,y_1,y_0&lt;2^m\)</span> such that <span class="math inline">\(2^{m}x_1+x_0=x\)</span><span class="math inline">\(2^{m}y_1+y_0=y\)</span></p>
<p>Then it follows that <span class="math inline">\(xy=2^{2m}x_1y_1+2^m(x_1y_0+x_0y_1)+x_0y_0\)</span></p>
<p>There are 4 multiplications to be performed here, and one method of multiplication has you do them all individually; then one does those multiplications by the same technique, and so on until the multiplications are all single-bit, upon which you do them by the usual method. Suppose it takes <span class="math inline">\(T(n)\)</span> basic operations (single-bit additions and multiplications) to multiply two <span class="math inline">\(n\)</span>-bit numbers by this method; then this method gives <span class="math inline">\(T(n)=4T(\left\lceil{}\frac{n}{2}\right\rceil{})+6n\)</span>, as we have to add 4 <span class="math inline">\(2n\)</span>-bit numbers, which require 3 additions (so require <span class="math inline">\(6n\)</span> basic operations in total).</p>
<p>This is called a recurrence relation, and can be solved by through a variety of means. For the sake of brevity, we shall use the master theorem, which tells us that, where <span class="math inline">\(T(n)=aT(\frac{n}{b})+f(n)\)</span>, and <span class="math inline">\(c_{crit}=\log_ba\)</span> (the <em>critical exponent</em>): when <span class="math inline">\(f(n)=O(n^c\)</span> for some <span class="math inline">\(c&lt;c_{crit}\)</span> (i.e. the work to split and recombine a problem is far less than that of the subproblems), <span class="math inline">\(T(n)=\Theta(n^{c_{crit}})\)</span>; when <span class="math inline">\(f(n)=\Theta(n^{c_{crit}}log^kn)\)</span> for some <span class="math inline">\(k\geq0\)</span> (i.e. the work to split and recombine a problem is comparable to that of the subproblems), <span class="math inline">\(T(n)=\Theta(n^{c_{crit}}log^{k+1}n)\)</span>; and when <span class="math inline">\(f(n)=\Omega(n^c)\)</span> for some <span class="math inline">\(c&gt;c_{crit}\)</span> (i.e. the work to split and recombine a problem is far greater than that of the subproblems), <span class="math inline">\(T(n)=\Theta(f(n))\)</span>.</p>
<p>In the case of the algorithm we are currently considering, <span class="math inline">\(T(n)=4T(\frac{n}{2})+6n\)</span> (we have dropped the ceiling signs, as they make a tiny change in the grand scheme of things to <span class="math inline">\(\frac{n}{2}\)</span> which does not affect the time taken significantly). Thus <span class="math inline">\(c_{crit}=\log_{2}4=2\)</span> and <span class="math inline">\(f(n)=6n=O(n^1)\)</span> so <span class="math inline">\(c=1&lt;c_{crit}\)</span>. The master theorem then tells us that <span class="math inline">\(T(n)=\Theta(n^2)\)</span>, which is no better than long multiplication.</p>
<p>What Karatsuba hit upon was that <span class="math inline">\(xy=2^{2m}x_1y_1+2^m(x_1y_0+x_0y_1)+x_0y_0\)</span> could be calculated with only 3 subproblems. We calculate <span class="math inline">\(z_2=x_1y_1\)</span> and <span class="math inline">\(z_0=x_0y_0\)</span>. Then we calculate <span class="math inline">\(z_1=(x_1+x_0)(y_1+y_0)-z_2-z_0\)</span>, which is equal to <span class="math inline">\(x_1y_0+x_0y_1\)</span>. Thus we have only made three multiplications, and a couple of additions and subtractions, both of which have linear time complexity. Thus Karatsuba’s algorithm gives us <span class="math inline">\(T(n)=3T(n/2)+f(n)\)</span>, where <span class="math inline">\(f(n)=O(n)\)</span>. By the master theorem, <span class="math inline">\(c_{crit}=\log_{2}3\)</span>, so as <span class="math inline">\(f(n)=O(n^1)\)</span>, <span class="math inline">\(c&lt;c_{crit}\)</span>, and <span class="math inline">\(T(n)=\Theta(n^{\log_{2}3})\approx{}\Theta(n^{1.585})\)</span>.</p>
<p>This means that in our previous example of two ten-million-digit numbers, we would be done in a couple of hours. This is a marked improvement, but we can still do better.</p>
<h1 id="toom-cook">Toom-Cook</h1>
<p>In 1963, Andrei Toom described a generalisation of the Karatsuba algorithm, which Stephen Cook improved as his PhD in 1966. It uses the technique of splitting numbers up (as in Karatsuba) and treating them as polynomials, which is also used in the Schönhage-Strassen algorithm, which is faster still.</p>
<p>Long multiplication is Toom-1, and has time complexity <span class="math inline">\(=O(n^2)\)</span>. The Karatsuba algorithm is Toom-2, and has time complexity <span class="math inline">\(\approx{}O(n^1.585)\)</span>. Toom-3 has time complexity <span class="math inline">\(\approx{}O(n^1.465)\)</span>. In general, Toom-<span class="math inline">\(k\)</span> has time complexity <span class="math inline">\(\Theta(c(k)n^{log(2k-1)/log(k)})\)</span>, where <span class="math inline">\(c(k)\)</span> is the time spent on addition and multiplication by small constants. Unfortunately, the function <span class="math inline">\(c(k)\)</span> grows very quickly. Through use of a variety of values of <span class="math inline">\(k\)</span>, Donald Knuth has made a Toom-Cook implementation that runs in <span class="math inline">\(\Theta(n2^{\sqrt{2\log{}n}}\log{}n)\)</span> Due to overhead, Toom-Cook is slower than long multiplication for small numbers. We shall consider Toom-3 as an example; however, we shall give the general algorithm of Toom-<span class="math inline">\(k\)</span> for any value of <span class="math inline">\(k\)</span>.</p>
<p>Given <span class="math inline">\(n\)</span>-digit numbers <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (we’ll use <span class="math inline">\(31415926\)</span> and <span class="math inline">\(2718281\)</span>), we start by splitting each into <span class="math inline">\(k\)</span> digits in some base <span class="math inline">\(B=b^i\)</span> for some integer <span class="math inline">\(i\)</span>, where we have the numbers in base <span class="math inline">\(b\)</span>.</p>
<p>Let these <span class="math inline">\(k\)</span> digits of <span class="math inline">\(x\)</span> be the coefficients of a polynomial <span class="math inline">\(P\)</span>, and those of <span class="math inline">\(y\)</span> be coefficients of a polynomial <span class="math inline">\(Q\)</span>. These polynomials have degree <span class="math inline">\(k-1\)</span>. Then <span class="math inline">\(Q(B)=x\)</span> and <span class="math inline">\(Q(B)=y\)</span>. Call the coefficients of <span class="math inline">\(P\)</span> <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_0\)</span>, such that <span class="math inline">\(x_i\)</span> is the coefficient of <span class="math inline">\(n^i\)</span>. Similarly, call the coefficients of <span class="math inline">\(Q\)</span> <span class="math inline">\(y_2\)</span>, <span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_0\)</span>. If we let <span class="math inline">\(B=1000\)</span> in our example:</p>
<p><span><span class="math inline">\(P(n)=31n^2+415n+926 \implies P(B)=31415926=x\)</span></span></p>
<p><span class="math inline">\(Q(n)=2n^2+718n+281 \implies Q(B)=2718281=y\)</span></p>
<p>We then want to find the product polynomial <span class="math inline">\(R(n)=P(n)Q(n)\)</span>, which will be of the form <span class="math inline">\(R(n)=z_4n^4+z_3n^3+z_2n^2+z_1n+z_0\)</span>, as we will then be able to easily evaluate <span class="math inline">\(R(B)=P(B)Q(B)=xy\)</span>, as the terms will be easy to calculate using the bit shift mentioned in the section on long multiplication. We do this by interpolation; we find <span class="math inline">\(2k-1\)</span> values of <span class="math inline">\(R(n)\)</span>, and work out the coefficients from that. Where the values of <span class="math inline">\(n\)</span> are small, the values of <span class="math inline">\(P(n)\)</span> and <span class="math inline">\(Q(n)\)</span> are easy to calculate. We abuse notation by saying that <span class="math inline">\(P(\infty)\)</span> is the value of the highest-degree coefficient of <span class="math inline">\(P\)</span>. For Toom-3, we shall therefore use <span class="math inline">\(-1\)</span>, 0, 1, 2, and <span class="math inline">\(\infty\)</span>.</p>
<p><span class="math inline">\(R(-1)=z_4-z_3+z_2-z_1+z_0\)</span></p>
<p><span class="math inline">\(R(0)=z_0\)</span></p>
<p><span class="math inline">\(R(1)=z_4+z_3+z_2+z_1+z_0\)</span></p>
<p><span class="math inline">\(R(2)=16z_4+8z_3+4z_2+2z_1+z_0\)</span></p>
<p><span class="math inline">\(R(\infty)=z_4\)</span></p>
<p>We can calculate all of these values by multiplying <span class="math inline">\(P(n)\)</span> and <span class="math inline">\(Q(n)\)</span>; this uses five multiplications of numbers of length <span class="math inline">\(\frac{n}{3}\)</span>. The coefficients of <span class="math inline">\(R\)</span> can be figured out from the five values of <span class="math inline">\(R(n)\)</span> with simple algebra thus:</p>
<p><span class="math inline">\(R(\infty)=z_4\)</span></p>
<p><span class="math inline">\(R(0)=z_0\)</span></p>
<p><span class="math inline">\(R(1)+R(-1)=2z_4+2z_2+2z_0\)</span></p>
<p><span class="math inline">\(\frac{R(1)+R(-1)}{2}=z_4+z_2+z_0\)</span></p>
<p><span class="math inline">\(\frac{R(1)+R(-1)}{2}-z_4-z_0=z_2\)</span></p>
<p><span class="math inline">\(R(1)-R(-1)=2z_3+2z_1\)</span></p>
<p><span class="math inline">\(R(2)-16z_4-4z_2-z_0=8z_3+2z_1\)</span></p>
<p><span class="math inline">\(R(2)-16z_4-4z_2-z_0-(R(1)-R(-1))=6z_3\)</span></p>
<p><span class="math inline">\(\frac{R(2)-16z_4-4z_2-z_0-(R(1)-R(-1))}{6}=z_3\)</span></p>
<p><span class="math inline">\(\frac{R(1)-R(-1)}{2}=z_3+z_1\)</span></p>
<p><span class="math inline">\(\frac{R(1)-R(-1)}{2}-z_3=z_1\)</span></p>
<p>From this we have found the coefficients of <span class="math inline">\(R\)</span>; we can now easily use bit shifts to evaluate <span class="math inline">\(R(B)=P(B)Q(B)=xy\)</span>. Thus we have only used 5 multiplications of length <span class="math inline">\(\frac{n}{3}\)</span> to calculate a multiplication of length <span class="math inline">\(n\)</span>, with some linear time complexity calculations in between. This means that by the master theorem from earlier, Toom-3 is <span class="math inline">\(O(n^{log_{3}5})\)</span>.</p>
<h1 id="faster-algorithms">Faster algorithms</h1>
<p>In 1971, Arnold Schönhage and Volker Strassen created the Schönhage-Strassen algorithm, which has time complexity <span class="math inline">\(O(n\log{}n\log(\log{}n))\)</span>. It is faster than Toom-Cook for integers greater than <span class="math inline">\(2^{2^17}\)</span>, that is to say, those with 40,000 decimal digits. It essentially splits the numbers into digits in some base, takes the discrete Fourier transform or them, multiplies the remaining vectors element by element, and then takes the inverse discrete Fourier transform, and conducts carrying.</p>
<p>The Fürer algorithm was discovered in 2007; it has time complexity <span class="math inline">\(O(2^{O(\log*n)}n\log{}n)\)</span>, and is better than Schönhage-Strassen for integers above <span class="math inline">\(2^{2^{64}}\)</span>. An implementation was made in 2015 with running time <span class="math inline">\(O(2^{3\log*n}n\log{}n)\)</span>, thus removing the nested big-O notation.</p>
<h1 id="problems">Problems</h1>
<ol>
<li><p>Design a sorting algorithm and work out its time complexity. Try to find the most efficient one you can!</p></li>
<li><p>Arnold Schönhage and Volker Strassen conjecture that the lower bound of time complexity for multiplication is <span class="math inline">\(\Omega(n\log{}n)\)</span>. Prove this.</p></li>
</ol>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This phrasing is a slight oversimplification, but helpful to consider in order to understand the concept.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>binary digit<a href="#fnref2">↩</a></p></li>
</ol>
</div>


        </article>
        <aside class="meta topics">



<h3><a class='nav' href='https://librarian.cf/s.html'>Sciences and Mathematics</a></h3><figure>
<a href=https://librarian.cf/articles/3/3.html
 class='nav'>
<h4 id="church-of-england-research-youth-views-on-technology-in-survey-of-chaplains-breakfast-joshua-loo">Church of England research youth views on technology in survey of Chaplain's Breakfast / <strong>Joshua Loo</strong></h4>
</a>
</figure><figure>
<a href=https://librarian.cf/articles/5/5.html
 class='nav'>
<h4 id="how-to-write-a-librarian-article-joshua-loo">How to write a Librarian article / <strong>Joshua Loo</strong></h4>
</a>
</figure>

        </aside>
        <aside class="fill"></aside>
    </main>
</body>

</html>

